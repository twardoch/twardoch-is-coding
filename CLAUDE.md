---
# this_file: CLAUDE.md
---

IMPORTANT: DO NOT ADD "enterprise code". DO NOT ADD "validation" or "security checks" or other code that ONLY BLOATS THE CODEBASE. FOCUS ON THE MINIMAL VIABLE NEXT VERSION. FOCUS ON THE CORE FUNCTIONALITY. THIS IS YOUR PRIME DIRECTIVE. TEST FREQUENTLY, and TEST WITH REAL DATA, not MOCK DATA. Do not fear using real APIs. 


# Project Rationale & Task Analysis: "Coding with LLMs" Conference

## 1. üéØ **Project Rationale**

This repository documents practical experience with LLM-assisted development tools and presents findings to experienced developers who've managed perfectly well without AI so far, thank you very much. Based on analysis of 177+ repositories and several thousand hours of actual usage rather than breathless blog posts.

### 1.1. **The Problem**
Experienced developers (particularly those who've been coding since before "web-scale" was a thing) need straightforward guidance on whether these LLM tools are worth the fuss. The current information landscape oscillates between academic papers that assume you have a PhD in transformer mathematics and marketing copy that promises AI will write your code while you sip cocktails.

### 1.2. **What This Actually Offers**
- **Evidence-based assessment**: 177 repositories, 54 Cursor projects, 1000+ Claude Code sessions
- **Real project examples**: pdf22png, claif-packages, vttiro, boabro (actual work, not demos)
- **Practical perspective**: Technical depth without requiring fluency in attention mechanisms
- **Tool comparison**: IDE vs CLI approaches, because choosing tools matters

## 2. üìã **Task Description**

### 2.1. **Deliverables**

#### 2.1.1. **Conference Talk** (20 minutes)
- **Format**: Marpit slides for TeX/Context developers
- **Structure**: LLM basics ‚Üí IDE vs CLI tools ‚Üí MCP protocol ‚Üí Real experience
- **Timing**: 5+6+6+3 minutes (because conference schedules are immutable laws of physics)

#### 2.1.2. **Documentation Site**
- **Format**: MkDocs site that doesn't require JavaScript to read
- **Purpose**: Reference material for those who want details beyond "it works"
- **Content**: Technical explanations without the marketing fluff

#### 2.1.3. **Research Base**
- **Portfolio analysis**: What actually happened across 177 repositories
- **Tool evaluation**: Comparative assessment of different approaches
- **Workflow documentation**: What works, what doesn't, and why

### 2.2. **Technical Implementation**

#### 2.2.1. **Slides**
- **Marpit framework**: Because PowerPoint is for people who enjoy suffering
- **Mermaid diagrams**: Visual explanations that actually explain things
- **PDF export**: For those who print slides (you know who you are)

#### 2.2.2. **Documentation**
- **MkDocs**: Static site generation without the complexity industrial complex
- **Material theme**: Clean, readable, works on mobile
- **GitHub Pages**: Hosting that costs nothing and breaks rarely

#### 2.2.3. **Build System**
- **Automated everything**: Because manual deployment is a solved problem from 2010
- **Cross-platform**: Works on Windows (yes, really), macOS, and Linux
- **Quality checks**: Automated validation so humans can focus on content

## 3. üîç **Project Scope**

### 3.1. **What This Is:**
- **Practical guide**: How to use LLM tools without drinking the Kool-Aid
- **Experience report**: What actually works after using these tools extensively
- **Tool comparison**: Honest assessment of different approaches
- **Conference content**: Technical presentation for working developers

### 3.2. **What This Is Not:**
- **Academic research**: No peer review or statistical significance testing
- **Official documentation**: Not sanctioned by any vendor
- **Course material**: Not designed for beginners or certification
- **Prophet of AI future**: Just reporting on tools that exist today

## 4. üé™ **Current Status**

### 4.1. **Infrastructure: Done** ‚úÖ
- ‚úÖ **Build system**: Automated, reliable, works across platforms
- ‚úÖ **Content framework**: Slides and documentation structure complete
- ‚úÖ **Quality tooling**: Validation, optimization, deployment sorted
- ‚úÖ **Research**: Portfolio analysis complete (all 177 repositories survived)

### 4.2. **Content: In Progress** üéØ
- üéØ **Slide polish**: Making 20 minutes actually informative
- üéØ **Documentation depth**: Beyond "this tool exists and is good"
- üéØ **Demo preparation**: Live coding that won't embarrass anyone
- üéØ **Rehearsal**: Ensuring timing doesn't require divine intervention

## 5. üöÄ **Immediate Tasks**

Focus: **CONTENT** over infrastructure (which already works).

### 5.1. **Priority 1: Slide Content**
- **Narrative flow**: Make 20 minutes worth attending
- **Technical clarity**: Explain without dumbing down
- **Demo backup**: Prepare for Murphy's Law of live coding
- **Visual coherence**: Diagrams that clarify rather than decorate

### 5.2. **Priority 2: Documentation Substance**
- **Technical depth**: Beyond marketing speak and into actual implementation
- **Working examples**: Code that runs, not pseudo-code that looks nice
- **Practical guides**: Instructions someone can actually follow
- **Progressive complexity**: From basic usage to advanced workflows

### 5.3. **Priority 3: Content Coherence**
- **Consistency**: Slides and docs should tell the same story
- **Accuracy**: All claims backed by actual testing
- **Currency**: Information that's still true by conference time
- **Flow**: Logical progression from concept to implementation

## 6. üéì **Success Criteria**

### 6.1. **Presentation Success**
- ‚úÖ Content exists and builds without errors
- üéØ Audience learns something useful in 20 minutes
- üéØ Live demos work (or fail gracefully)
- üéØ Q&A doesn't reveal fundamental ignorance

### 6.2. **Documentation Value**
- ‚úÖ Site serves as reference after talk
- üéØ Someone can actually implement these tools after reading
- üéØ Information stays accurate longer than typical tech blog posts
- üéØ Useful to people who weren't at the conference

---

## 7. üìù **Summary**

This project documents practical experience with LLM development tools and presents findings to experienced developers. It combines conference presentation with comprehensive documentation, focusing on evidence-based assessment rather than speculation or marketing.

**Current focus**: Content quality and practical value. Infrastructure works; now make the content worth the time investment.

**Differentiator**: Based on actual usage across 177 repositories rather than vendor demos or theoretical frameworks. Suitable for developers who need practical guidance without requiring advanced degrees in machine learning. 

---

I‚Äôm a simple skeptic (but positive). To me, LLMs are a natural evolution and clever mechanisms for lossy data compression and prediction that results from this. They're very very useful, but they're not revolutionary in my view. In our work here, we tend to tone down empty praise. We may employ some very understated unmarked clever UK-style humor sporadically aimed at those who get it, but if someone doesn't get it, then it's OK too. 